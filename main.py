# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UvZp9p1V-sBmzu-jnYiZzgt4xzAYwz7B
"""

!pip -q install requests beautifulsoup4 lxml pandas gspread google-auth

from google.colab import files
uploaded = files.upload()  # ÿßÿÆÿ™ÿßÿ± service_account.json

SHEET_ID = "1sPTMv_YgzTpUyyOVdlD0jsuMlyAVF0K4tlK4WeMB9Hs"

import os
print(os.listdir())

import time, re
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup
import pandas as pd

import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime, timezone

# ====== CONFIG ======
SERVICE_ACCOUNT_FILE = "adept-coda-484817-i2-8988bc17dafb.json"
BASE_URL = "https://books.toscrape.com/"
REQUEST_DELAY = 0.6

# ====== AUTH GOOGLE SHEETS ======
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]
creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
gc = gspread.authorize(creds)

sh = gc.open_by_key(SHEET_ID)
ws = sh.sheet1

# ====== HELPERS ======
def get_session():
    s = requests.Session()
    s.headers.update({"User-Agent": "Mozilla/5.0 Chrome/120 Safari/537.36"})
    return s

def request(session, url):
    r = session.get(url, timeout=20)
    r.raise_for_status()
    return r.text

def parse_price(text):
    m = re.search(r"([\d]+\.\d+)", text)
    return float(m.group(1)) if m else None

def rating_to_int(cls_list):
    mapping = {"One":1, "Two":2, "Three":3, "Four":4, "Five":5}
    for c in cls_list:
        if c in mapping:
            return mapping[c]
    return None

def get_categories(session):
    html = request(session, BASE_URL)
    soup = BeautifulSoup(html, "lxml")
    cats = []
    for a in soup.select(".side_categories ul li ul li a"):
        name = a.get_text(strip=True)
        url = urljoin(BASE_URL, a.get("href"))
        cats.append((name, url))
    return cats

def scrape_all():
    session = get_session()
    categories = get_categories(session)
    rows = []

    for cat_name, cat_url in categories:
        page_url = cat_url
        while True:
            html = request(session, page_url)
            soup = BeautifulSoup(html, "lxml")

            for art in soup.select("article.product_pod"):
                a = art.select_one("h3 a")
                title = a.get("title")
                product_url = urljoin(page_url, a.get("href"))

                price = parse_price(art.select_one(".price_color").get_text())
                availability = art.select_one(".availability").get_text(" ", strip=True)
                rating = rating_to_int(art.select_one(".star-rating").get("class", []))

                rows.append({
                    "title": title,
                    "price_gbp": price,
                    "availability": availability,
                    "rating": rating,
                    "category": cat_name,
                    "url": product_url,
                })

            next_btn = soup.select_one("li.next a")
            if not next_btn:
                break
            page_url = urljoin(page_url, next_btn.get("href"))
            time.sleep(REQUEST_DELAY)

    return rows

# ====== RUN ======
print("‚è≥ Scraping started...")
rows = scrape_all()
df = pd.DataFrame(rows)
df["scraped_at"] = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")

print("‚úç Writing to Google Sheet...")
ws.clear()
ws.update([df.columns.tolist()] + df.astype(str).values.tolist())

print(f"‚úÖ DONE! Rows scraped: {len(df)} ‚Äî Check your Google Sheet üî•")
df.head()